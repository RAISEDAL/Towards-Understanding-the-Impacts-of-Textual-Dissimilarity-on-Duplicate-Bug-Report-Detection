{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries \n",
        "import pandas as pd\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import numpy as np\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from typing import List, Dict, Tuple, Set\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import pickle\n",
        "import requests\n",
        "import json\n",
        "import urllib.request\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import statistics\n",
        "from datetime import datetime\n",
        "from scipy.stats import skew\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "metadata": {
        "id": "YBc2CtGYZTCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mounting the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GXIfn-rvu7pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading dataset"
      ],
      "metadata": {
        "id": "77GiYleYVlax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the CSV file of the dataset from google drive into a dataframe\n",
        "train_eclipse_df=pd.read_csv('drive/MyDrive/Project 6308 + ICSME/Siamese/Mobile_siamese_train_preprocessed.csv')\n",
        "train_eclipse_df"
      ],
      "metadata": {
        "id": "a82T-YTgU1nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the CSV file of the dataset from google drive into a dataframe\n",
        "test_eclipse_df=pd.read_csv('drive/MyDrive/Project 6308 + ICSME/Siamese/Mobile_siamese_test_preprocessed.csv')\n",
        "test_eclipse_df"
      ],
      "metadata": {
        "id": "FKNb9p7tVkhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge train & test\n",
        "frames = [train_eclipse_df, test_eclipse_df]\n",
        "result = pd.concat(frames)\n",
        "result = result.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "result1 = result.copy()\n",
        "dup_df= result.copy()\n",
        "dup_df"
      ],
      "metadata": {
        "id": "Q9mDbnx-YOhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURES = ['description1', 'description2']\n",
        "print('Description of length of the feature columns')\n",
        "dup_df[FEATURES].apply(lambda col: col.str.len().describe())"
      ],
      "metadata": {
        "id": "iUe8hMYBY31i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train, Validation, Test Split"
      ],
      "metadata": {
        "id": "PAV0wX2jxSWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "ydZiq_CqxTVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_df = train_eclipse_df.copy()\n",
        "train_val_df"
      ],
      "metadata": {
        "id": "xAlQ7mFiVvax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " test_df = test_eclipse_df.copy()\n",
        " test_df"
      ],
      "metadata": {
        "id": "FjyZNriwxU8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(\n",
        "  train_val_df,\n",
        "  test_size=0.2,\n",
        "  stratify=train_val_df.is_similar,\n",
        "  random_state=13,\n",
        ")"
      ],
      "metadata": {
        "id": "n4RGc4NYxX3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train Val Test Size: {len(train_df):,} {len(val_df):,} {len(test_df):,}')"
      ],
      "metadata": {
        "id": "bRlsFfTExl6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download & Prepare Embedding"
      ],
      "metadata": {
        "id": "QWklXbxt0AfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.utils as kutils\n",
        "from keras.layers.preprocessing.text_vectorization import TextVectorization"
      ],
      "metadata": {
        "id": "6gm-gZy60BSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kutils==0.3.0"
      ],
      "metadata": {
        "id": "i2oZQzY-U_7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.utils as kutils\n",
        "from keras.layers.preprocessing.text_vectorization import TextVectorization"
      ],
      "metadata": {
        "id": "4YYlKDKKVvDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "Path('C:\\Program Files').parent"
      ],
      "metadata": {
        "id": "Nm1SHmvtouup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "4-MolO0rXjsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip\n"
      ],
      "metadata": {
        "id": "CO45TFi8ZGXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!pwd"
      ],
      "metadata": {
        "id": "9UPcOTcjZOt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "jzmKnP3eZUxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -U -q pydrive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "metadata": {
        "id": "dJ2WxB8RZfge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump({'embeddings_index' : embeddings_index } , open('drive/MyDrive/Project 6308 + ICSME/Siamese/glove.42B.300d.txt', 'wb'))"
      ],
      "metadata": {
        "id": "EzV5a4QSZyZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = open('drive/MyDrive/Project 6308 + ICSME/Siamese/glove.42B.300d.txt',  mode=\"r\", encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "q4ewGsRHgp9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import open"
      ],
      "metadata": {
        "id": "Mjpipqwvi-Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_glove_embedding(verbose=False):\n",
        "  import pathlib\n",
        "  # If this operation fails, print the parent-dir\n",
        "  # go there, and extract the file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_glove(path, words=None):\n",
        "    word_to_embedding = {open('drive/MyDrive/Project 6308 + ICSME/Siamese/glove.42B.300d.txt',  mode=\"r\", encoding=\"utf-8\")}\n",
        "\n",
        "    with iopen(path, 'r') as stream:\n",
        "        for n, line in enumerate(stream):\n",
        "            if not isinstance(line, str):\n",
        "                line = line.decode('utf-8')\n",
        "            split_line = line.split(' ')\n",
        "            word = split_line[0]\n",
        "\n",
        "            if words is None or word in words:\n",
        "                try:\n",
        "                    word_to_embedding[word] = [float(f) for f in split_line[1:]]\n",
        "                except ValueError:\n",
        "                    logger.error('{}\\t{}\\t{}'.format(n, word, str(split_line)))\n",
        "\n",
        "    return word_to_embedding \n",
        "\n",
        "glove_file_path = ensure_glove_embedding(verbose=True)"
      ],
      "metadata": {
        "id": "w70oBSi0Ywen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print('Indexing word vectors.')\n",
        "\n",
        "embedding_index = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embedding_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embedding_index))\n"
      ],
      "metadata": {
        "id": "vBRWmn7wXamR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create vocabulary index"
      ],
      "metadata": {
        "id": "0twgBVfbrJeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List"
      ],
      "metadata": {
        "id": "pikhDBmarXVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 20000\n",
        "MAX_TITLE_LENGTH = 21\n",
        "MAX_DESCRIPTION_LENGTH = 500\n",
        "EMBEDDING_DIM = 100"
      ],
      "metadata": {
        "id": "cItkSImjrL-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences: List[str], sequence_length: int):\n",
        "  vectorizer = TextVectorization(\n",
        "    max_tokens=MAX_TOKENS - 2,\n",
        "    output_sequence_length=sequence_length,\n",
        "  )\n",
        "  vectorizer.adapt(sentences)\n",
        "  vocab = vectorizer.get_vocabulary()\n",
        "  word_index = dict(zip(vocab, range(len(vocab))))\n",
        "\n",
        "  return vectorizer, word_index"
      ],
      "metadata": {
        "id": "GQqko955rNdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dup_df"
      ],
      "metadata": {
        "id": "FPWIJxkprpq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descr_vectorizer, descr_word_index = build_vocab(\n",
        "  [dup_df.description1, dup_df.description2],\n",
        "  MAX_DESCRIPTION_LENGTH,\n",
        ")\n",
        "\n",
        "print(\n",
        "  'Most frequent description words:',\n",
        "  list(itertools.islice(descr_word_index.keys(), 5)),\n",
        ")"
      ],
      "metadata": {
        "id": "VlWPcs0ArPg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Embedding Matrix"
      ],
      "metadata": {
        "id": "T0EDNUlKsDiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(\n",
        "  embeddings_index: Dict[str, np.ndarray],\n",
        "  word_index: Dict[str, int],\n",
        "  verbose=False,\n",
        "):\n",
        "  hits = 0\n",
        "  misses = 0\n",
        "\n",
        "  # prepare embedding matrix\n",
        "  embedding_matrix = np.zeros((MAX_TOKENS, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # Words not found in embedding index will be all-zeros.\n",
        "      # This includes the representation for \"padding\" and \"OOV\"\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      hits += 1\n",
        "    else:\n",
        "      misses += 1\n",
        "\n",
        "  if verbose:\n",
        "    print('Embedding shape:', embedding_matrix.shape)\n",
        "    print(f'Found {hits} words, missed {misses}.')\n",
        "\n",
        "  return embedding_matrix"
      ],
      "metadata": {
        "id": "r_ZZge2ysE2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Creating description embedding matrix:')\n",
        "descr_embedding_matrix = create_embedding_matrix(\n",
        "  embeddings_index, descr_word_index, True,\n",
        ")"
      ],
      "metadata": {
        "id": "9Ee7W0zSsILC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Training Data"
      ],
      "metadata": {
        "id": "LiVmA9BquIfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batches(\n",
        "  split_df: pd.DataFrame,\n",
        "  batch_size=256,\n",
        "):\n",
        "  steps_per_epoch = len(split_df) // batch_size\n",
        "  while True:\n",
        "    for i in range(steps_per_epoch):\n",
        "      offset = i * batch_size\n",
        "      till = offset + batch_size\n",
        "      feature_batches = []\n",
        "      for feature in FEATURES:\n",
        "        vectorizer = title_vectorizer if feature.startswith('title') else descr_vectorizer\n",
        "        feature_batch = vectorizer(\n",
        "          split_df[feature][offset: till].to_numpy().reshape((-1, 1))\n",
        "        ).numpy()\n",
        "        feature_batches.append(feature_batch)\n",
        "\n",
        "\n",
        "      target_batch = split_df.is_similar[offset: till].to_numpy()\n",
        "      yield (\n",
        "        feature_batches,\n",
        "        target_batch,\n",
        "      )"
      ],
      "metadata": {
        "id": "S0Rfq90VuJSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Models"
      ],
      "metadata": {
        "id": "0RmhmXHOuOum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import callbacks\n",
        "from keras.initializers.initializers_v2 import Constant"
      ],
      "metadata": {
        "id": "4cfVI21muPqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DescrEmbeddingLayer = layers.Embedding(\n",
        "  input_dim=MAX_TOKENS,\n",
        "  output_dim=EMBEDDING_DIM,\n",
        "  embeddings_initializer=Constant(descr_embedding_matrix),\n",
        "  trainable=False,\n",
        "  name='DescrEmbeddingLayer',\n",
        ")\n",
        "\n",
        "def make_descr_layer(num, kernel_size=3, pool_size=2, strides=None):\n",
        "  DescrConv1dLayer = layers.Conv1D(\n",
        "    filters=32,\n",
        "    kernel_size=kernel_size,\n",
        "    activation='relu',\n",
        "    name=f'DescrConv1dLayer{num}',\n",
        "  )\n",
        "  DescrMaxPool1dLayer = layers.MaxPool1D(\n",
        "    pool_size=pool_size,\n",
        "    strides=strides,\n",
        "    name=f'DescrMaxPool1dLayer{num}',\n",
        "  )\n",
        "  return DescrConv1dLayer, DescrMaxPool1dLayer\n",
        "\n",
        "DescrConv1dLayer1, DescrMaxPool1dLayer1 = make_descr_layer(1, pool_size=4)\n",
        "DescrConv1dLayer2, DescrMaxPool1dLayer2 = make_descr_layer(2)"
      ],
      "metadata": {
        "id": "iD1j962RuSTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_siamese_component(num: int):\n",
        "  class SiameseComponent:\n",
        "    def __init__(self, title: layers.Input, description: layers.Input, output: layers.Concatenate):\n",
        "      self.title = title\n",
        "      self.description = description\n",
        "      self.output = output\n",
        "\n",
        "  title_input = layers.Input(shape=(None,), dtype='int32', name=f'title{num}_input')\n",
        "\n",
        "  descr_input = layers.Input(shape=(None,), name=f'descr{num}_input')\n",
        "  descr_embedding_layer = DescrEmbeddingLayer(descr_input)\n",
        "  descr_conv1d1 = DescrConv1dLayer1(descr_embedding_layer)\n",
        "  descr_max_pool1d1 = DescrMaxPool1dLayer1(descr_conv1d1)\n",
        "  descr_conv1d2 = DescrConv1dLayer2(descr_max_pool1d1)\n",
        "  descr_max_pool1d2 = DescrMaxPool1dLayer2(descr_conv1d2)\n",
        "  # descr_conv1d3 = DescrConv1dLayer3(descr_max_pool1d2)\n",
        "  # descr_max_pool1d3 = DescrMaxPool1dLayer3(descr_conv1d3)\n",
        "  descr_flat_Layer = layers.Flatten(name=f'FlatDescr{num}')(descr_max_pool1d2)\n",
        "  \n",
        "  concat = layers.Concatenate(axis=1, name=f'Concat{num}')([descr_flat_Layer])\n",
        "  return SiameseComponent(title_input, descr_input, concat)\n",
        "\n",
        "\n",
        "component1 = create_siamese_component(1)\n",
        "component2 = create_siamese_component(2)\n",
        "\n",
        "dot_product_layer = layers.Dot(\n",
        "  axes=1,\n",
        "  name='dot_product_layer'\n",
        ")([component1.output, component2.output])\n",
        "output = layers.Dense(\n",
        "  1, activation='sigmoid', name='output',\n",
        ")(dot_product_layer)\n",
        "siamese_model = models.Model(\n",
        "  inputs=[component1.description, component2.description],\n",
        "  outputs=output,\n",
        "  name='siamese_model'\n",
        ")\n",
        "\n",
        "kutils.plot_model(siamese_model)"
      ],
      "metadata": {
        "id": "hKaKSfpwuWrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "BrWZRqkJwBtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "siamese_model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "   optimizer=opt,\n",
        "  metrics=['acc'],\n",
        ")\n",
        "callback = callbacks.ModelCheckpoint(\n",
        "    filepath=f'../../drive/MyDrive/Project 6308 + ICSME/Siamese'\n",
        "             '.epoch-{epoch:02d}-loss-{val_loss:.3f}.hdf5',\n",
        "    monitor='val_loss',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        ")"
      ],
      "metadata": {
        "id": "rBJFNAONwD_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = siamese_model.fit(\n",
        "  generate_batches(train_df, BATCH_SIZE),\n",
        "  steps_per_epoch=len(train_df) // BATCH_SIZE,\n",
        "  epochs=12,\n",
        "  validation_data=generate_batches(val_df, BATCH_SIZE),\n",
        "  validation_steps=len(val_df) // BATCH_SIZE,\n",
        "  verbose=1,\n",
        "  # callbacks= [callback]\n",
        ")"
      ],
      "metadata": {
        "id": "HRAOIDsQ2u0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = pd.DataFrame(history.history).plot()"
      ],
      "metadata": {
        "id": "UK6kHy_0xrN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "plot_model(siamese_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "# plot the loss\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')"
      ],
      "metadata": {
        "id": "ARFey1_L1Nff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "plt.plot(history.history['acc'], label='train acc')\n",
        "plt.plot(history.history['val_acc'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ],
      "metadata": {
        "id": "1sRgywWh4f6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on test data"
      ],
      "metadata": {
        "id": "uxivE_4KF-lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import load_model\n",
        "\n",
        "# siamese_model = load_model('../../models/siamese-openOffice.epoch-30-loss-0.457.hdf5')"
      ],
      "metadata": {
        "id": "1O6FnyjgF_EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.evaluate(\n",
        "  generate_batches(test_df, BATCH_SIZE),\n",
        "  steps=len(test_df) // BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "id": "hQ_Vlc2U1Sf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_df)"
      ],
      "metadata": {
        "id": "M9LG-i2a2HLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = len(test_df)\n",
        "num_batched_instances = len(test_df) - (len(test_df) % BATCH_SIZE)\n",
        "pred_y = siamese_model.predict(\n",
        "  generate_batches(test_df, BATCH_SIZE),\n",
        "  steps=len(test_df) // BATCH_SIZE,\n",
        "  verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "D_S1YT7gGC1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(\n",
        "  test_df.is_similar[:num_batched_instances],\n",
        "  pred_y > .5,\n",
        "))"
      ],
      "metadata": {
        "id": "5esiuBvyHOaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,roc_auc_score\n",
        "import numpy as np; np.random.seed(0)\n",
        "import seaborn as sns; sns.set_theme()\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn.metrics as metrics"
      ],
      "metadata": {
        "id": "R8OK-tLz5szV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "value_names = ['TPR', 'FPR', 'Threshold']\n",
        "roc = dict(zip(value_names, roc_curve(\n",
        "  test_df.is_similar[:num_batched_instances],\n",
        "  pred_y,\n",
        ")))\n",
        "pd.DataFrame(roc)"
      ],
      "metadata": {
        "id": "ivdeHQwIIQPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "auroc = roc_auc_score(\n",
        "  test_df.is_similar[:num_batched_instances],\n",
        "  pred_y,\n",
        ")\n",
        "\n",
        "print('AUROC score:', auroc)"
      ],
      "metadata": {
        "id": "hGbIeSFWITbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('For probability:')\n",
        "print(f'Mean: {pred_y.mean()}, STD: {pred_y.std()}')\n",
        "print('For categorical:')\n",
        "print(f'Mean: {(pred_y > .5).mean()}, STD: {(pred_y > .5).std()}')"
      ],
      "metadata": {
        "id": "hKRDSyhv-RB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on textually similar data"
      ],
      "metadata": {
        "id": "7i8FGP06ZtbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the CSV file of the dataset from google drive into a dataframe\n",
        "sim_df=pd.read_csv('drive/MyDrive/Project 6308 + ICSME/Siamese/Mobile_siamese_textually_similar_test_duplicates_preprocessed.csv')\n",
        "sim_df"
      ],
      "metadata": {
        "id": "ctv5i4uwZwQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = len(sim_df)\n",
        "num_batched_instances = len(sim_df) - (len(sim_df) % BATCH_SIZE)\n",
        "pred_y_sim = siamese_model.predict(\n",
        "  generate_batches(sim_df, BATCH_SIZE),\n",
        "  steps=len(sim_df) // BATCH_SIZE,\n",
        "  verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "weGMLu0eaM4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.evaluate(\n",
        "  generate_batches(sim_df, BATCH_SIZE),\n",
        "  steps=len(sim_df) // BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "id": "oe8168B8ztDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(\n",
        "  sim_df.is_similar[:num_batched_instances],\n",
        "  pred_y_sim > .5,\n",
        "))"
      ],
      "metadata": {
        "id": "apm5qUZtaTCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "value_names = ['TPR', 'FPR', 'Threshold']\n",
        "roc = dict(zip(value_names, roc_curve(\n",
        "  sim_df.is_similar[:num_batched_instances],\n",
        "  pred_y_sim,\n",
        ")))\n",
        "pd.DataFrame(roc)"
      ],
      "metadata": {
        "id": "i4UOiMGvaXq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "auroc = roc_auc_score(\n",
        "  sim_df.is_similar[:num_batched_instances],\n",
        "  pred_y_sim,\n",
        ")\n",
        "\n",
        "print('AUROC score:', auroc)\n"
      ],
      "metadata": {
        "id": "Uv4ot3cnaZy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('For probability:')\n",
        "print(f'Mean: {pred_y_sim.mean()}, STD: {pred_y_sim.std()}')\n",
        "print('For categorical:')\n",
        "print(f'Mean: {(pred_y_sim > .5).mean()}, STD: {(pred_y_sim > .5).std()}')"
      ],
      "metadata": {
        "id": "M6I1HvSh-lfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on textually dissimilar data"
      ],
      "metadata": {
        "id": "BmRmorjEfQb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the CSV file of the dataset from google drive into a dataframe\n",
        "dissim_df=pd.read_csv('drive/MyDrive/Project 6308 + ICSME/Siamese/Mobile_siamese_textually_dissimilar_test_duplicates_preprocessed.csv')\n",
        "dissim_df"
      ],
      "metadata": {
        "id": "S5np3yqwfV0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = len(dissim_df)\n",
        "num_batched_instances = len(dissim_df) - (len(dissim_df) % BATCH_SIZE)\n",
        "pred_y_dis = siamese_model.predict(\n",
        "  generate_batches(dissim_df, BATCH_SIZE),\n",
        "  steps=len(dissim_df) // BATCH_SIZE,\n",
        "  verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "i1-3kMw_fh-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.evaluate(\n",
        "  generate_batches(dissim_df, BATCH_SIZE),\n",
        "  steps=len(dissim_df) // BATCH_SIZE,\n",
        ")"
      ],
      "metadata": {
        "id": "w4_X1A4gzwD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(\n",
        "  dissim_df.is_similar[:num_batched_instances],\n",
        "  pred_y_dis > .5,\n",
        "))"
      ],
      "metadata": {
        "id": "Xqhsl_Byfm6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "value_names = ['TPR', 'FPR', 'Threshold']\n",
        "roc = dict(zip(value_names, roc_curve(\n",
        "  dissim_df.is_similar[:num_batched_instances],\n",
        "  pred_y_dis,\n",
        ")))\n",
        "pd.DataFrame(roc)"
      ],
      "metadata": {
        "id": "UC2L7lDofqqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "auroc = roc_auc_score(\n",
        "  dissim_df.is_similar[:num_batched_instances],\n",
        "  pred_y_dis,\n",
        ")\n",
        "\n",
        "print('AUROC score:', auroc)\n"
      ],
      "metadata": {
        "id": "keUtQxHbfv2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('For probability:')\n",
        "print(f'Mean: {pred_y_dis.mean()}, STD: {pred_y_dis.std()}')\n",
        "print('For categorical:')\n",
        "print(f'Mean: {(pred_y_dis > .5).mean()}, STD: {(pred_y_dis > .5).std()}')"
      ],
      "metadata": {
        "id": "xhSh7qzv0aT2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}